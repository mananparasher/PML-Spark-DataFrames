{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Tutorials\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a Json file in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a Parquet file in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"filname.paraquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer a schema in Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_schema = types.StructType([\n",
    "    types.StructField('page', types.StringType(), False),\n",
    "    types.StructField('page_title', types.StringType(), False),\n",
    "    types.StructField('page_voews', types.IntegerType(), False),\n",
    "    types.StructField('page_size', types.IntegerType(), False),])\n",
    "\n",
    "data = spark.read.format('csv').load(inputs, schema=observation_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show a Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|name |\n",
      "+---+-----+\n",
      "|25 |Sethu|\n",
      "|26 |Manan|\n",
      "|25 |Shree|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select a column in Spark DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Sethu|\n",
      "|Manan|\n",
      "|Shree|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union of two DataFrames in Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25|Sethu|\n",
      "| 26|Manan|\n",
      "| 25|Shree|\n",
      "| 25|Sethu|\n",
      "| 26|Manan|\n",
      "| 25|Shree|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "union_example=df.union(df)\n",
    "union_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split a column in Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|grades_column|\n",
      "+-------------+\n",
      "|            A|\n",
      "|            A|\n",
      "|            B|\n",
      "|            B|\n",
      "|            B|\n",
      "|            B|\n",
      "|            A|\n",
      "|            C|\n",
      "|            B|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df2=spark.read.json(\"data2.json\")\n",
    "explode_example = df2.select(explode(\"grades\").alias(\"grades_column\"))\n",
    "explode_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter row in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "|age|   grades|   name|\n",
      "+---+---------+-------+\n",
      "| 24|[B, B, B]|Kashish|\n",
      "+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_example=df2.filter(df2.name==\"Kashish\")\n",
    "filter_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where clause in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "|age|   grades|   name|\n",
      "+---+---------+-------+\n",
      "| 24|[B, B, B]|Kashish|\n",
      "+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "where_example=df2.where(df2.name==\"Kashish\")\n",
    "where_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+\n",
      "|age|   grades|   name|\n",
      "+---+---------+-------+\n",
      "| 23|[A, C, B]| Slavvy|\n",
      "| 24|[B, B, B]|Kashish|\n",
      "| 25|[A, A, B]|  Ankit|\n",
      "+---+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting_example=df2.sort(df2.age)\n",
    "sorting_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling null values in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25|Sethu|\n",
      "| 26|Manan|\n",
      "| 25|Shree|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fill_null_values=df.fillna(\"--\")\n",
    "fill_null_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| name|count|\n",
      "+-----+-----+\n",
      "|Sethu|    2|\n",
      "|Manan|    2|\n",
      "|Shree|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupby_Example=union_example.groupby(union_example.name).count()\n",
    "groupby_Example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum of a column in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|sum(age)|\n",
      "+---+--------+\n",
      "| 25|      25|\n",
      "| 23|      23|\n",
      "| 24|      24|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_example=df2.groupby(df2.age).sum().alias(\"sum\")\n",
    "sum_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---------+-----+\n",
      "|age| name|age|   grades| name|\n",
      "+---+-----+---+---------+-----+\n",
      "| 25|Shree| 25|[A, A, B]|Ankit|\n",
      "| 25|Sethu| 25|[A, A, B]|Ankit|\n",
      "+---+-----+---+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join_example=df.join(df2,(df.age==df2.age))\n",
    "join_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order by in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "| 25|Shree|\n",
      "| 25|Sethu|\n",
      "| 26|Manan|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orderby_example=df.orderBy(df.age)\n",
    "orderby_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add a new column in Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "|age| name|new_col|\n",
      "+---+-----+-------+\n",
      "| 25|Sethu|      1|\n",
      "| 26|Manan|      1|\n",
      "| 25|Shree|      1|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "new_column_example=df.withColumn(\"new_col\",lit(1))\n",
    "new_column_example.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
